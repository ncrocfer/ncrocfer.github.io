<!doctype html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Posts &middot; Nicolas Crocfer</title>

    <meta name="description" content="Python Developer">

    <meta name="generator" content="Hugo 0.42.2" />
    <meta name="twitter:card" content="summary">
    
    <meta name="twitter:title" content="Posts &middot; Nicolas Crocfer">
    <meta name="twitter:description" content="Python Developer">

    <meta property="og:type" content="article">
    <meta property="og:title" content="Posts &middot; Nicolas Crocfer">
    <meta property="og:description" content="Python Developer">

    <link href='//fonts.googleapis.com/css?family=Source+Sans+Pro:400,700|Oxygen:400,700' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/pure/0.6.0/pure-min.css">
    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/pure/0.6.0/grids-responsive-min.css">

    <link rel="stylesheet" href="https://ncrocfer.github.io/css/all.min.css">
    <link rel="stylesheet" href="https://ncrocfer.github.io/css/flexboxgrid.min.css">
    <link rel="stylesheet" href="https://ncrocfer.github.io/css/custom.css">
    <link href="//maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css" rel="stylesheet">

    <link rel="alternate" type="application/rss+xml" title="Nicolas Crocfer" href="https://ncrocfer.github.io/index.xml" />
</head>
<body>



<div id="layout" class="pure-g">
    <div class="sidebar pure-u-1 pure-u-md-1-4">
    <div class="header">
        <hgroup>
            <h1 class="brand-title"><a href="https://ncrocfer.github.io">Nicolas Crocfer</a></h1>
            <h2 class="brand-tagline"> Python Developer </h2>
        </hgroup>

        <nav class="nav">
            <ul class="nav-list">
                
                <li class="nav-item">
                    <a class="pure-button" href="https://twitter.com/ncrocfer">
                        <i class="fa fa-twitter"></i> Twitter
                    </a>
                </li>
                
                
                <li class="nav-item">
                    <a class="pure-button" href="https://github.com/ncrocfer">
                        <i class="fa fa-github-alt"></i> Github
                    </a>
                </li>
                
                
                
                <li class="nav-item">
                    <a class="pure-button" href="https://www.linkedin.com/in/nicolascrocfer">
                        <i class="fa fa-linkedin"></i> LinkedIn
                    </a>
                </li>
                
                <li class="nav-item">
                    <a class="pure-button" href="https://ncrocfer.github.io/index.xml">
                        <i class="fa fa-rss"></i> rss
                    </a>
                </li>
            </ul>
        </nav>
    </div>
</div>


    <div class="content pure-u-1 pure-u-md-3-4">
        <div>
            
            <div class="posts">
                
                <h1 class="content-subhead">09 Feb 2020, 13:04</h1>
                <section class="post">
                    <header class="post-header">

                        <a href="https://ncrocfer.github.io/posts/celery-utiliser-filesystem/" class="post-title">Celery : utiliser le filesystem comme broker et result backend</a>

                        <p class="post-meta">
                            
                            
                        </p>
                    </header>

                    <div class="post-description">
                        <p>Lorsqu&rsquo;on utilise Celery en production il est d&rsquo;usage d&rsquo;utiliser un <strong>broker</strong> robuste comme RabbitMQ. Dans le cas où nos tâches utilisent un canvas tel que <code>chord</code> il faudra également penser à configurer la connection vers un <strong>result backend</strong> afin de stocker le résultat des tâches. On pourra par exemple utiliser Redis.</p>

<p>Mais avant la mise en production il y a l&rsquo;étape du développement des tâches. Et même si l&rsquo;installation d&rsquo;un Redis ou d&rsquo;un RabbitMQ est assez simple, j&rsquo;ai pu remarqué par le passé que ces dépendances fortes pouvaient parfois être un frein à l&rsquo;adoption de Celery.</p>

<p>Hors il faut savoir que Celery est basé sur la librairie <a href="https://github.com/celery/kombu">Kombu</a>, utilisée pour transmettre des messages d&rsquo;un producer à un consumer. Et la bonne nouvelle est que Kombu <a href="https://kombu.readthedocs.io/en/latest/_modules/kombu/transport/filesystem.html">supporte le filesystem</a> comme mode de transport !</p>

<p>Ce qui signifie qu&rsquo;en configurant Celery on peut facilement s&rsquo;abstraire de ces dépendances :</p>

<pre><code class="language-python">from celery import Celery

app = Celery('tasks')
app.conf.update({
    # Configuration du broker
    &quot;broker_url&quot;: &quot;filesystem://&quot;,
    &quot;broker_transport_options&quot;: {
        &quot;data_folder_in&quot;: &quot;/tmp/out&quot;,
        &quot;data_folder_out&quot;: &quot;/tmp/out&quot;,
    },

    # Configuration du result backend
    &quot;result_backend&quot;: &quot;file:///tmp/result&quot;,
})
</code></pre>

<p>Comme vous le voyez c&rsquo;est très simple :</p>

<ul>
<li>on indique via le <code>broker_url</code> que nous souhaitons utiliser le système de fichiers,</li>
<li>puis on transmet à Kombu le dossier dans lequel les messages seront stockés.</li>
</ul>

<p>Evidemment les clés <code>data_folder_in</code> et <code>data_folder_out</code> doivent être les mêmes, sinon vos producers écriront leurs messages dans un dossier et vos consumers tenteront de les lire ailleurs.</p>

<p>Et comme Celery <a href="https://docs.celeryproject.org/en/latest/internals/reference/celery.backends.filesystem.html">supporte par défaut</a> le filesystem comme backend, sa configuration peut se faire via la clé <code>result_backend</code>.</p>

<p>Il est important que les dossiers existent avant de lancer Celery, voici donc un petit snippet très simple qui les créera pour vous sous un dossier <code>.celery</code> du répertoire courant :</p>

<pre><code class="language-python"># tasks.py
import os
from celery import Celery

OUT_DIR = os.path.join(os.getcwd(), &quot;.celery/out&quot;)
RESULT_DIR = os.path.join(os.getcwd(), &quot;.celery/result&quot;)

# Create folders if they don't exist
for dir in [OUT_DIR, RESULT_DIR]:
    if not os.path.exists(dir):
        os.makedirs(dir)


app = Celery('tasks')
app.conf.update({
    &quot;broker_url&quot;: &quot;filesystem://&quot;,
    &quot;broker_transport_options&quot;: {
        &quot;data_folder_in&quot;: OUT_DIR,
        &quot;data_folder_out&quot;: OUT_DIR,
    },
    &quot;result_backend&quot;: f&quot;file://{RESULT_DIR}&quot;,
})

@app.task
def tsum(numbers):
    return sum(numbers)
</code></pre>

<p>On peut tester en lançant un simple chord :</p>

<pre><code>In [1]: from tasks import tsum

In [2]: from celery import chord

In [3]: chord(
   ...:     [tsum.si([1, 2]), tsum.si([3, 4])],
   ...:     tsum.s()
   ...: ).delay()
Out[3]: &lt;AsyncResult: 11422611-c389-4020-9358-ca485b14d65d&gt;
</code></pre>

<p>Puis en visualisant les résultats du worker :</p>

<pre><code>$ celery@LAPTOP v4.4.0 (cliffs)

Darwin-17.7.0-x86_64-i386-64bit 2020-02-09 14:11:36

[config]
.&gt; app:         tasks:0x10605a8d0
.&gt; transport:   filesystem://localhost//
.&gt; results:     file:///Users/ncrocfer/Dev/.celery/result
.&gt; concurrency: 4 (prefork)
.&gt; task events: OFF (enable -E to monitor tasks in this worker)

[queues]
.&gt; celery           exchange=celery(direct) key=celery


[tasks]
  . tasks.tsum

[2020-02-09 14:11:36,645: INFO/MainProcess] Connected to filesystem://localhost//
[2020-02-09 14:11:36,677: INFO/MainProcess] celery@LAPTOP ready.
[2020-02-09 14:11:36,682: INFO/MainProcess] Received task: celery.chord_unlock[71b7898d-b3b4-4689-9f4b-ebbe2e96cb1d]  ETA:[2020-02-09 13:10:35.235858+00:00]
[2020-02-09 14:11:36,687: INFO/MainProcess] Received task: tasks.tsum[39a3728b-ee9a-44f8-b661-97e20e74c72e]
[2020-02-09 14:11:36,694: INFO/MainProcess] Received task: tasks.tsum[e79e3917-1c80-4366-adcc-23b601f38836]
[2020-02-09 14:11:36,700: INFO/ForkPoolWorker-1] Task tasks.tsum[39a3728b-ee9a-44f8-b661-97e20e74c72e] succeeded in 0.005183988017961383s: 3
[2020-02-09 14:11:36,705: INFO/ForkPoolWorker-2] Task tasks.tsum[e79e3917-1c80-4366-adcc-23b601f38836] succeeded in 0.005367834935896099s: 7
[2020-02-09 14:11:37,737: INFO/ForkPoolWorker-3] Task celery.chord_unlock[71b7898d-b3b4-4689-9f4b-ebbe2e96cb1d] succeeded in 0.05950633704196662s: None
[2020-02-09 14:11:38,711: INFO/MainProcess] Received task: tasks.tsum[11422611-c389-4020-9358-ca485b14d65d]
[2020-02-09 14:11:38,719: INFO/ForkPoolWorker-4] Task tasks.tsum[11422611-c389-4020-9358-ca485b14d65d] succeeded in 0.0034185299882665277s: 10
</code></pre>

<p>Evidemment cette configuration n&rsquo;est à utiliser qu&rsquo;en mode <em>développement</em>, je vous conseille fortement de passer sous un autre broker lorsque vous passerez vos tâches en prod :)</p>

                    </div>
                </section>
                
                <h1 class="content-subhead">12 Jan 2020, 13:25</h1>
                <section class="post">
                    <header class="post-header">

                        <a href="https://ncrocfer.github.io/posts/celery-importance-pool-execution/" class="post-title">Celery : l&#39;importance du pool d&#39;exécution</a>

                        <p class="post-meta">
                            
                            
                        </p>
                    </header>

                    <div class="post-description">
                        

<p>En tant que dev nous sommes régulièrement amenés à devoir exécuter des tâches en arrière-plan. La plupart du temps il s&rsquo;agira de répondre à une action de l&rsquo;utilisateur (call d&rsquo;API, click sur un bouton&hellip;), parfois nous aurons besoin de lancer une tâche de manière périodique.</p>

<p>Dans les 2 cas l&rsquo;objectif est de ne pas bloquer le processus principal. En effet je vous laisse imaginer les pertes de performance si, au moment d&rsquo;un POST sur votre API, votre application se chargeait d&rsquo;exécuter elle-même une tâche lourde et très longue. L&rsquo;application rendrait la main à l&rsquo;utilisateur plusieurs secondes, voire plusieurs minutes, après le call.</p>

<img src="/images/celery.png" alt="" style="display: block; margin-left: auto; margin-right: auto; ">

<p>Pour répondre à ce besoin les dev Python ont pris l&rsquo;habitude d&rsquo;utiliser la librairie <strong>Celery</strong>. Cet article n&rsquo;est pas une introduction à cet outil, pour cela je vous renvoie vers le <a href="https://docs.celeryproject.org/en/latest/getting-started/first-steps-with-celery.html">quickstart</a> officiel, vous comprendrez rapidement à quoi il sert et surtout comment l&rsquo;utiliser.</p>

<p>Nous allons parler ici d&rsquo;une feature qui est parfois délaissée lorsqu&rsquo;on utilise Celery : les <strong>pools d&rsquo;exécution</strong>. Derrière cette notion se cache toute la magie de Celery : comment et par quoi mes tâches vont-elles s&rsquo;exécuter. Et vous allez voir que configurer correctement cette option peut amener des gains de performances incroyables !</p>

<h1 id="la-théorie">La théorie</h1>

<p>Lorsque nous lançons un worker Celery, nous lançons en fait un processus chargé de coordonner le lancement des tâches empilées dans le broker. En réalité leur exécution à proprement parler ne sera pas effectuée par ce processus mais par d&rsquo;autres processus enfants ou par des threads qu&rsquo;il aura lui-même spawné.</p>

<p>Et c&rsquo;est justement la façon dont est configuré le pool d&rsquo;éxécution qui détermine comment sont lancés ces processus ou threads capables d&rsquo;exécuter les tâches.</p>

<p>Celery fournit par défaut 4 pools d&rsquo;exécution : <strong>prefork</strong>, <strong>solo</strong>, <strong>gevent</strong> et <strong>eventlet</strong>. La configuration s&rsquo;effectue via l&rsquo;option <code>-P</code>.</p>

<h2 id="mode-prefork">Mode prefork</h2>

<p>Par défault le pool sélectionné par Celery est le prefork. Nous pouvons facilement le voir lorsque nous lançons un worker :</p>

<pre><code>$ celery -A tasks worker
celery@LAPTOP v4.4.0 (cliffs)

Darwin-17.7.0-x86_64-i386-64bit 2020-01-26 14:31:06

[config]
.&gt; app:         tasks:0x10566ca20
.&gt; transport:   redis://localhost:6379/0
.&gt; results:     disabled://
.&gt; concurrency: 4 (prefork)
.&gt; task events: OFF (enable -E to monitor tasks in this worker)

[queues]
.&gt; celery           exchange=celery(direct) key=celery
</code></pre>

<p>Ici la ligne <code>concurrency: 4 (prefork)</code> nous indique que nous utilisons le pool <code>prefork</code> avec une concurrency de <strong>4</strong>. Ce mode d&rsquo;exécution se base sur le package <a href="https://docs.python.org/fr/3/library/multiprocessing.html">multiprocessing</a> : Celery va donc spawner autant de processus que le nombre de coeurs de votre machine.</p>

<p>Nous pouvons vérifier que les processus ont bien été spawnés par Celery (le process principal et les 4 forks):</p>

<pre><code>$ ps -A | grep -i celery
91822 ttys000    0:01.59 /Users/ncrocfer/Dev/celery_pool/venv/bin/python3 /Users/ncrocfer/Dev/celery_pool/venv/bin/celery -A tasks worker
91828 ttys000    0:00.02 /Users/ncrocfer/Dev/celery_pool/venv/bin/python3 /Users/ncrocfer/Dev/celery_pool/venv/bin/celery -A tasks worker
91829 ttys000    0:00.02 /Users/ncrocfer/Dev/celery_pool/venv/bin/python3 /Users/ncrocfer/Dev/celery_pool/venv/bin/celery -A tasks worker
91830 ttys000    0:00.02 /Users/ncrocfer/Dev/celery_pool/venv/bin/python3 /Users/ncrocfer/Dev/celery_pool/venv/bin/celery -A tasks worker
91831 ttys000    0:00.02 /Users/ncrocfer/Dev/celery_pool/venv/bin/python3 /Users/ncrocfer/Dev/celery_pool/venv/bin/celery -A tasks worker
</code></pre>

<p>Ce mode est particulièrement adapté aux applications nécessitant du calcul par le CPU (ces applications sont dîtes <strong>CPU bound</strong>). Plus vous aurez de coeurs puissants et plus votre application pourra traiter de tâches en parallèle.</p>

<p>A noter que la concurrency est paramétrable via l&rsquo;option <code>-c</code> :</p>

<pre><code>$ celery -A tasks worker -c 2
...
.&gt; concurrency: 2 (prefork)
...
</code></pre>

<h2 id="mode-solo">Mode solo</h2>

<p>Ce mode est un peu particulier car le processus principal se chargera lui-même d&rsquo;exécuter les tâches qu&rsquo;il consommera :</p>

<pre><code>$ celery -A tasks worker -P solo
...
.&gt; concurrency: 4 (solo)
...
</code></pre>

<p>La concurrency reste à 4 mais ne vous y fiez pas, le mode <strong>solo</strong> ne s&rsquo;en encombre pas et n&rsquo;exécutera qu&rsquo;une seule tâche à la fois.</p>

<p>Là encore nous pouvons le vérifier via les processus en cours d&rsquo;exécution sur notre machine :</p>

<pre><code>$ ps -A | grep -i celery
92356 ttys000    0:01.13 /Users/ncrocfer/Dev/celery_pool/venv/bin/python3 /Users/ncrocfer/Dev/celery_pool/venv/bin/celery -A tasks worker -P solo
</code></pre>

<p>Hormis pour lancer des tests, ce mode pourra être utile dans le cas d&rsquo;un usage en environnement <em>containers</em>. En effet les comptes sont simples dans ce cas : <em>N</em> containers égal <em>N</em> tasks en parallèle.</p>

<h2 id="mode-gevent-eventlet">Mode gevent &amp; eventlet</h2>

<p>Là on arrive dans la partie intéressante :) A l&rsquo;inverse du mode <code>prefork</code> utile pour gérer des tâches <strong>CPU bound</strong>, ces deux modes vont nous permettre de gérer facilement les tâches dîtes <strong>IO bound</strong>.</p>

<p>De telles tâches n&rsquo;effectuent pas de calculs lourds, et ne nécessitent donc pas un grand nombre de CPU puissants. En revanche elles peuvent être amenées à effectuer beaucoup de requêtes vers d&rsquo;autres systèmes : une API externe, une base de données, un site web, etc. Ici notre CPU n&rsquo;a pas grand chose à faire puisque nos tâches vont principalement attendre le retour de leurs requêtes.</p>

<p>Je vous invite à vous renseigner sur la librairie <a href="https://docs.python.org/3/library/asyncio.html">Asyncio</a> qui expliquera tout ça mieux que moi.</p>

<p>Dans l&rsquo;exemple ci-dessous notre worker pourra gérer 100 tâches en parallèle via le mode <code>gevent</code> :</p>

<pre><code>$ celery -A tasks worker -P gevent -c 100
...
.&gt; concurrency: 100 (gevent)
...
</code></pre>

<h1 id="la-pratique">La pratique</h1>

<p>Trève de théorie, illustrons maintenant l&rsquo;utilisation des pool d&rsquo;exécution via un exemple.</p>

<p>Nous souhaitons lancer plusieurs tâches en parallèle effectuant une requête vers une API externe. Cette API renvoie un nombre au hasard, et une fonction de callback est appelée afin d&rsquo;effectuer la somme de ces nombres. Certes cet exemple est basique mais il va nous permettre de mettre en évidence la puissance des pools d&rsquo;exécution dans Celery.</p>

<p>L&rsquo;API tourne en locale, j&rsquo;ai utilisé <a href="https://sanic.readthedocs.io/en/latest/">Sanic</a> avec un timeout de 2 secondes <em>simulant</em> la latence d&rsquo;une API externe :</p>

<pre><code class="language-python">$ cat app.py
import random
import asyncio

from sanic import Sanic
from sanic.response import json


app = Sanic()


@app.get(&quot;/&quot;)
async def get_time(request):
    await asyncio.sleep(2)
    return json({&quot;number&quot;: random.randint(1, 10)})


if __name__ == &quot;__main__&quot;:
    app.run(host=&quot;0.0.0.0&quot;, port=8000)
</code></pre>

<p>Nos tâches sont elles-aussi très simples :</p>

<pre><code class="language-python">$ cat tasks.py
import requests
from celery import Celery, chord


app = Celery('tasks', broker='redis://localhost:6379/0', backend=&quot;redis://localhost:6379/1&quot;)


@app.task
def get_number():
    resp = requests.get(&quot;http://localhost:8000&quot;)
    return resp.json()[&quot;number&quot;]

@app.task
def sum_numbers(numbers):
    return sum(numbers)


sign = chord(
    [get_number.si() for _ in range(5)],
    sum_numbers.s()
)
</code></pre>

<p>J&rsquo;utilise un chord afin de lancer plusieurs requêtes en parallèle. La fonction <code>sum_numbers</code> est ensuite lancée afin de calculer la somme totale des nombres.</p>

<p>Le lancement peut s&rsquo;effectuer en une ligne :</p>

<pre><code>$ python -c &quot;from tasks import sign; sign.delay()&quot;
</code></pre>

<p>Testons avec le pool d&rsquo;exécution par défaut (prefork) :</p>

<pre><code>$ celery -A tasks worker --loglevel=INFO
...
[2020-01-26 19:30:48,852: INFO/MainProcess] Received task: tasks.get_number[5f2f8167-c667-4e83-b591-453e57b3a958]
[2020-01-26 19:30:48,855: INFO/MainProcess] Received task: tasks.get_number[a333586b-c396-4b91-917f-3fbdfa6d30e8]
[2020-01-26 19:30:48,860: INFO/MainProcess] Received task: tasks.get_number[88fa7cc0-2622-4053-8824-2f2d47c25a04]
[2020-01-26 19:30:48,876: INFO/MainProcess] Received task: tasks.get_number[fa35004e-51b8-4f48-ad84-e02640ed9621]
[2020-01-26 19:30:48,880: INFO/MainProcess] Received task: tasks.get_number[5ecb73c8-9ba5-4ebb-aaae-78e14908405a]
[2020-01-26 19:30:50,915: INFO/ForkPoolWorker-2] Task tasks.get_number[5f2f8167-c667-4e83-b591-453e57b3a958] succeeded in 2.057117607037071s: 1
[2020-01-26 19:30:50,915: INFO/ForkPoolWorker-3] Task tasks.get_number[a333586b-c396-4b91-917f-3fbdfa6d30e8] succeeded in 2.057576177001465s: 2
[2020-01-26 19:30:50,920: INFO/ForkPoolWorker-1] Task tasks.get_number[88fa7cc0-2622-4053-8824-2f2d47c25a04] succeeded in 2.0558868430089206s: 2
[2020-01-26 19:30:50,933: INFO/ForkPoolWorker-4] Task tasks.get_number[fa35004e-51b8-4f48-ad84-e02640ed9621] succeeded in 2.055466585967224s: 1
[2020-01-26 19:30:52,972: INFO/ForkPoolWorker-2] Task tasks.get_number[5ecb73c8-9ba5-4ebb-aaae-78e14908405a] succeeded in 2.0525691980146803s: 10
[2020-01-26 19:30:52,973: INFO/MainProcess] Received task: tasks.sum_numbers[605e4af4-229c-42a3-9be0-ce5daff79a0f]
[2020-01-26 19:30:52,975: INFO/ForkPoolWorker-2] Task tasks.sum_numbers[605e4af4-229c-42a3-9be0-ce5daff79a0f] succeeded in 0.0006177640170790255s: 16
</code></pre>

<p>Les tâches sont toutes prises en compte en une seule fois par notre worker (rien à voir avec la notion de pool d&rsquo;exécution, il s&rsquo;agit ici du <strong>prefetch</strong>, en gros le nombre de tâches qu&rsquo;un worker peut se <em>réserver</em>).</p>

<p>En revanche seules 4 tâches sont effectuées en même temps, il faudra attendre 2 secondes supplémentaires pour que la 5ème le soit également. Le workflow complet aura donc pris <strong>4 secondes</strong>.</p>

<p>Testons maintenant avec le pool d&rsquo;exécution <strong>gevent</strong> :</p>

<pre><code>$ celery -A tasks worker --loglevel=INFO -P gevent -c 100
...
[2020-01-26 19:35:12,151: INFO/MainProcess] Received task: tasks.get_number[04192b64-b71e-4e1c-8eb8-f86487c6b013]
[2020-01-26 19:35:12,163: INFO/MainProcess] Received task: tasks.get_number[12d227b5-f0b1-4fd8-8b44-25e8d8fbc028]
[2020-01-26 19:35:12,173: INFO/MainProcess] Received task: tasks.get_number[db7838f5-3543-414c-9816-7df62b13b65d]
[2020-01-26 19:35:12,182: INFO/MainProcess] Received task: tasks.get_number[52f163d4-7d75-493a-a3ac-65122691b078]
[2020-01-26 19:35:12,193: INFO/MainProcess] Received task: tasks.get_number[14537419-8710-4b32-bb13-6c884ff22dca]
[2020-01-26 19:35:14,202: INFO/MainProcess] Task tasks.get_number[04192b64-b71e-4e1c-8eb8-f86487c6b013] succeeded in 2.048632029967848s: 6
[2020-01-26 19:35:14,207: INFO/MainProcess] Task tasks.get_number[12d227b5-f0b1-4fd8-8b44-25e8d8fbc028] succeeded in 2.042411718983203s: 2
[2020-01-26 19:35:14,208: INFO/MainProcess] Task tasks.get_number[14537419-8710-4b32-bb13-6c884ff22dca] succeeded in 2.0140742670046166s: 10
[2020-01-26 19:35:14,209: INFO/MainProcess] Task tasks.get_number[db7838f5-3543-414c-9816-7df62b13b65d] succeeded in 2.03493124502711s: 6
[2020-01-26 19:35:14,225: INFO/MainProcess] Task tasks.get_number[52f163d4-7d75-493a-a3ac-65122691b078] succeeded in 2.0403319080360234s: 9
[2020-01-26 19:35:14,225: INFO/MainProcess] Received task: tasks.sum_numbers[043e87a0-4963-468a-9f17-7ffe93127fcd]
[2020-01-26 19:35:14,227: INFO/MainProcess] Task tasks.sum_numbers[043e87a0-4963-468a-9f17-7ffe93127fcd] succeeded in 0.0006761000258848071s: 33
</code></pre>

<p>Ici les 5 tâches sont bien effectuées en même temps, le workflow aura donc mis <strong>2 secondes</strong>. Et encore je n&rsquo;ai lancé que 5 tâches en parallèle, n&rsquo;hésitez pas à augmenter ce nombre afin de constater par vous-même les résultats. On comprend bien la puissance de Gevent face à des tâches dédiées à de l&rsquo;IO Bound.</p>

<h1 id="conclusion">Conclusion</h1>

<p>Les pool d&rsquo;exécution ne sont pas beaucoup mis en avant dans la documentation Celery, hors comme vous le voyez il s&rsquo;agit d&rsquo;une configuration très simple à mettre en oeuvre afin de bénéficier de meilleurs performances.</p>

<p>N&rsquo;hésitez surtout pas à utiliser les queues pour effectuer le routing de vos tâches IO bound et CPU bound. Les tâches gourmandes en CPU pourront être redirigées vers une queue <strong>cpu</strong> et les tâches gourmandes en IO bound vers une queue <strong>io</strong>.</p>

<p>Il suffira alors de lancer 2 workers avec les bons arguments :</p>

<pre><code>$ celery -A tasks worker -P gevent -c 100 -Q io
$ celery -A tasks worker -P prefork -c 4 -Q cpu
</code></pre>

<p>Vous trouverez plus d&rsquo;info sur les queues <a href="https://docs.celeryproject.org/en/latest/userguide/routing.html">ici</a>.</p>

<p>En espérant que cet article vous ait plus, n&rsquo;hésitez pas à me contacter sur <a href="https://twitter.com/ncrocfer">Twitter</a> si vous avez des questions ou des remarques :)</p>

                    </div>
                </section>
                
                <h1 class="content-subhead">27 Oct 2019, 23:44</h1>
                <section class="post">
                    <header class="post-header">

                        <a href="https://ncrocfer.github.io/posts/retour-experience-lancement-projet-opensource/" class="post-title">Retour d&#39;experience sur le lancement d&#39;un projet opensource</a>

                        <p class="post-meta">
                            
                            
                        </p>
                    </header>

                    <div class="post-description">
                        <p>Ceux qui me suivent sur <a href="https://twitter.com/ncrocfer">Twitter</a> savent que j&rsquo;ai créé il y a quelque temps une plateforme dédiée à l&rsquo;alerting de vulnérabilités : <a href="https://www.saucs.com">Saucs.com</a>.</p>

<p>Déjà 3 ans (le premier commit date du 03/11/2016) que le projet vit et que de nombreux utilisateurs s&rsquo;en servent pour effectuer leur veille au quotidien. Malheureusement la vie personnelle et professionnelle fait qu&rsquo;il est très difficile de maintenir seul ce genre de projet, c&rsquo;est pourquoi il a été décidé en 2019 d&rsquo;opensourcer Saucs.</p>

<p>Cet article explique les difficultés d&rsquo;un tel projet.</p>

<p></p>

<h1 id="pourquoi-opensourcer-saucs">Pourquoi opensourcer Saucs</h1>

<p>Pour info la plateforme s&rsquo;appelait à l&rsquo;origine SecAdvisory, mais nous avons décidé de la renommer lorsque <a href="https://twitter.com/LaurentDurnez">Laurent</a> a pris part au projet en tant que sysadmin. Il a depuis fait un boulot de fou, et si le site tient si bien la charge c&rsquo;est vraiment grâce à lui et à l&rsquo;architecture qu&rsquo;il a su mettre en place.</p>

<p>A la base Saucs est né d&rsquo;un besoin personnel : je souhaitais être tenu au courant des vulnérabilités d&rsquo;un produit mais aucune plateforme ne répondait à mes attentes. L&rsquo;idée a donc été de parser les CVE mises à disposition par le NVD, d&rsquo;en extraire les produits associés (grâce aux CPE) puis de proposer un système d&rsquo;alertes basé dessus.</p>

<p>En soit le principe est extrèmement simple, mais de nombreux utilisateurs ont été séduits par Saucs, peut-être dû au fait que le NVD ne fournit pas cette fonctionnalité sur son propre site. Les chiffres actuels sont plutôt sympas : au moment de l&rsquo;écriture de cet article nous comptons exactement 3,351 utilisateurs et 1,184,274 rapports envoyés.</p>

<p>Malheureusement nous ne sommes pas assez et nous n&rsquo;avons pas le temps de tout gérer. Alors que nous avons des tonnes d&rsquo;idées en tête, leur concrétisation est devenu très compliquée par simple manque de temps. Les fonctionnalités ne sortent plus à un rythme convenable, et c&rsquo;est pourquoi nous avons donc décidé début 2019 de libérer le code source et de transformer Saucs en solution opensource.</p>

<p>A priori l&rsquo;idée est excellente : un code maintenu par la communauté, plus de fonctionnalités, d&rsquo;autres outils basés dessus, la possibilité d&rsquo;installation on-premise, etc.</p>

<p>Mais c&rsquo;est bien plus facile à dire qu&rsquo;à faire, et je vais vous expliquer dans cet article pourquoi. En outre les utilisateurs réclamant à tout bout de champs où j&rsquo;en suis comprendront peut-être un peu mieux pourquoi cela met autant de temps :)</p>

<h1 id="les-difficultés-rencontrées">Les difficultés rencontrées</h1>

<p>La principale difficulté que j&rsquo;ai rencontré a été de me <em>replonger</em> dans mon propre code source ! Cela faisait plusieurs mois que je n&rsquo;y avais pas touché (voire plus d&rsquo;un an), et la reprise a été difficile.</p>

<p>Il a ensuite fallu décider ce que je devais garder et ce que je devais modifier. La liste des choses à changer était finalement assez courte : le thème (indispensable car acheté en ligne et sous licence), l&rsquo;upgrade des dépendances, un peu de refacto par-ci par-là, et c&rsquo;était à priori tout. Ce qui pouvait me paraître simple m&rsquo;a finalement pris plusieurs soirées et week-ends sur mon temps libre (pour rappel ce projet n&rsquo;est en aucun cas lié à mon job).</p>

<p>Ce travail de refacto a malheureusement fait apparaître un nouveau problème : le legacy accumulé en 3 ans. En effet l&rsquo;écosystème entourant les CVE avait sacrément évolué, et les flux proposés par le NVD aussi. Les fichiers XML sur lesquels Saucs se basait sont désormais dépréciés par un flux JSON. Encore une fois, ce qui peut paraître simple (&ldquo;il suffit de changer le parser, voyons&rdquo;) ne l&rsquo;est pas forcément quand ça n&rsquo;a pas été prévu (dépendances fortes dans le code).</p>

<p>Anecdote amusante : j&rsquo;ai entamé la migration du XML vers le JSON il y a quelques mois. A ce moment-là le NVD fournissait la version 1.0 de ce flux, et je me servais d&rsquo;une clé nommée <em>affects</em> pour parser les vendeurs et leurs produits affectés. Cette clé était très pratique puisqu&rsquo;elle fournissait directement la liste dans un format simple. Malheureusement j&rsquo;ai eu la mauvaise surprise en revenant de vacances de constater que mon code levait une exception&hellip; la clé avait été supprimée dans la nouvelle version 1.1 de ce flux (<a href="https://nvd.nist.gov/General/News/JSON-1-1-Vulnerability-Feed-Release">changelog</a>):</p>

<blockquote>
<p><em>&ldquo;In CVE_JSON_4.0_min.schema, the affects element has been removed from the required properties.&rdquo;</em></p>
</blockquote>

<p>Encore du travail non prévu. Je pourrai également citer l&rsquo;exemple des CVSS : Saucs était à l&rsquo;origine basé sur la version 2 alors que la version 3 est désormais proposée. Un changement dans le model Python (et donc dans la database directement) a dû être opéré, impliquant des migrations dans le schema actuel.</p>

<p>Le refacto du code m&rsquo;a également permis de détecter ce que je pourrais appeler aujourd&rsquo;hui <em>des erreurs de conception</em>. Erreurs qui me paraissaient normales et non gênantes il y a 3 ans. C&rsquo;est là qu&rsquo;on remarque que notre façon de coder évolue dans le temps :)</p>

<p>Pour finir il est également important de noter que le code ne pouvait pas être opensourcer tel quel car l&rsquo;installation était pour ainsi dire <em>chaotique</em>. En effet l&rsquo;import initial des CVE, CPE et CWE prenait plusieurs heures, il a donc fallut travailler à l&rsquo;amélioration de cette partie. Pour info c&rsquo;est désormais faisable en quelques minutes grâce à la commande <code>saucs import</code>.</p>

<h1 id="les-erreurs-à-éviter">Les erreurs à éviter</h1>

<p>Je dirai que la principale erreur que j&rsquo;ai commise a été d&rsquo;annoncer l&rsquo;opensource de Saucs avant même d&rsquo;avoir commencé à travailler dessus.</p>

<p>Je ne savais pas la charge de travail que cela allait représenter, et j&rsquo;ai créé une attente parmi certains utilisateurs. Il faut savoir que je reçois régulièrement des messages sur Twitter de personnes, et parfois d&rsquo;entreprises, qui me demandent où j&rsquo;en suis. La plupart de ces messages sont bienveillants. Les utilisateurs souhaitent juste l&rsquo;utiliser chez eux en local, ou alors certaines entreprises me contactent pour affiner leur propre roadmap. Néanmoins je reçois parfois des messages pouvant se résumer à : <em>&ldquo;bah alors t&rsquo;en es où de l&rsquo;opensource ?? Ca traine !&rdquo;</em>. Ceux-là me font sourire, et dans ces cas-là je leur rappelle tout simplement que je ne leur dois rien.</p>

<p>Dans tous les cas je vous conseille de bien à réfléchir à votre roadmap avant d&rsquo;annoncer quoi que ce soit, l&rsquo;attente créée peut être énorme et on vous demandera tôt ou tard des comptes.</p>

<p>Ma deuxième erreur a été de vouloir trop en faire. Je me suis lancé dans un refacto complet du code, ce qui a impliqué beaucoup de changements que je n&rsquo;avais pas prévu. J&rsquo;ai finalement décidé de stopper ces évolutions avant l&rsquo;opensource et de les proposer après. Qui sait, peut-être que ce sera la communauté elle-même qui les créera. J&rsquo;en serai ravi !</p>

<h1 id="and-next">And next ?</h1>

<p>L&rsquo;ajout de la version 3 des CVSS a rajouté un bug lors de l&rsquo;envoie des mails, j&rsquo;ai découvert ça ce week-end et je dois le fixer. Une fois cela fait nous allons écrire les scripts pour migrer les données de la base actuelle vers le nouveau schema. Seulement à ce moment là, lorsque la nouvelle version sera disponible en ligne sur <a href="https://www.saucs.com">Saucs.com</a>, le code source pourra être libéré sur Github.</p>

<p>J&rsquo;ai hâte que ce moment arrive, d&rsquo;une part car je sais que l&rsquo;attente a été longue et que certains d&rsquo;entres vous sont impatients, et d&rsquo;autre part car des développeurs m&rsquo;ont déjà proposé leur aide pour de futures fonctionnalités.</p>

<p>Nous avons vraiment envie que ce projet serve au plus grand nombre, et cela ne peut passer que par l&rsquo;opensource. C&rsquo;est la dernière ligne droite, ça devient bon !</p>
                    </div>
                </section>
                
                <h1 class="content-subhead">10 Dec 2018, 10:00</h1>
                <section class="post">
                    <header class="post-header">

                        <a href="https://ncrocfer.github.io/posts/introduction-a-la-base-de-donnees-orientee-graphe-neo4j/" class="post-title">Introduction à la base de données orientée graphe Neo4j</a>

                        <p class="post-meta">
                            
                            
                        </p>
                    </header>

                    <div class="post-description">
                        <p>Je travaille depuis un an et demi sur un projet chez OVH qui calcule la qualité de service de notre infrastructure.</p>

<p>Cet outil, nommé DEPC, se compose d&rsquo;une API développée en Flask ainsi que d&rsquo;une WebUI. Le fonctionnement interne repose sur 3 composants majeurs :</p>

<ul>
<li>Apache Airflow pour le scheduling,</li>
<li>les base de données TimeSeries pour le calcul de la QOS,</li>
<li>et enfin la base de données Neo4j pour la gestion des dépendances.</li>
</ul>

<p>Je ne vais pas présenter DEPC dans cet article, nous allons bientôt l&rsquo;open sourcer et j&rsquo;en parlerai à ce moment-là. En revanche laissez-moi vous présenter une base données que j&rsquo;utilise désormais au quotidien et que j&rsquo;affectionne tout particulièrement : <strong>Neo4j</strong>.</p>

<p><img src="/images/neo4j_logo.png" alt="" style="display: block; margin-left: auto; margin-right: auto; width: 40%;">
</p>

<h1 id="base-de-données-orientée-graphe">Base de données orientée graphe</h1>

<p>En tant que développeur nous sommes habitués à travailler avec des bases de données relationnelles, que ce soit PostgreSQL, MySQL ou encore Oracle pour n&rsquo;en citer que quelques unes (<a href="https://en.wikipedia.org/wiki/List_of_relational_database_management_systems">la liste est longue</a>).</p>

<p>Pour rappel l&rsquo;élément principal d&rsquo;une base de données relationnelle est la <strong>table</strong> : les données qui y sont stockées sont organisées sous forme de tableau où chaque ligne correspond à un enregistrement et chaque colonne à une catégorie de même type.</p>

<p>Ce format est parfait pour la plupart de nos usages, notamment grâce à leur propriété <a href="https://en.wikipedia.org/wiki/ACID_(computer_science)">ACID</a>. Néanmoins les limites sont vite atteintes dans certains cas, notamment lorsqu&rsquo;il s&rsquo;agit de réaliser plusieurs jointures en une seule requête.</p>

<p>Les bases de données orientées graphe ont donc été créées pour pallier entre autres à ce problème. Et on peut dire qu&rsquo;elles connaissent un succès grandissant depuis quelques années (<a href="https://db-engines.com/en/ranking_categories">source</a>):</p>

<img src="/images/databases_model_trends.png" alt="" style="display: block; margin-left: auto; margin-right: auto; width: 70%;">

<p>Et ça tombe bien puisque Neo4j est la base de données orientées graphe la plus répandue !</p>

<h1 id="intérêts-des-graphes">Intérêts des graphes</h1>

<p>Un graphe représente un ensemble de points reliés entre eux par des arcs (<a href="https://fr.wikipedia.org/wiki/Th%C3%A9orie_des_graphes">source de l&rsquo;image</a>):</p>

<img src="/images/graphe.png" alt="" style="display: block; margin-left: auto; margin-right: auto; width: 70%;">

<p>L&rsquo;intérêt des graphes devient évident lorsque l&rsquo;on souhaite visualiser les intéractions entre différentes données.</p>

<p>Prenons le cas d&rsquo;un projet Python nécessitant de nombreuses librairies. Certaines de ces librairies nécessitent elles-même d&rsquo;autres librairies, et ainsi de suite. Il est très difficile avec pip de retrouver qui dépend de qui.</p>

<p>Le projet <a href="https://github.com/scrapy/scrapy">Scrapy</a> par exemple requiert une vingtaine de librairies :</p>

<pre><code class="language-bash">$ pip install scrapy
$ pip freeze | wc -l
25
</code></pre>

<p>Vous le savez la sortie de <code>pip freeze</code> nous renvoie la liste des modules <em>à plat</em>, sans aucune arborescence. Il est alors très difficile de visualiser quelle librairie dépend d&rsquo;une autre.</p>

<p>Nous pouvons utiliser l&rsquo;outil <a href="https://github.com/naiquevin/pipdeptree">pipdeptree</a> afin de générer un arbre de dépendances de nos modules :</p>

<pre><code class="language-bash">$ pip install pipdeptree graphviz
$ pipdeptree --graph-output png &gt; scrapy.png
</code></pre>

<p>Nous obtenons le graphe suivant, bien plus lisible que le résultat de pip :</p>

<img src="/images/scrapy_dependencies.png" alt="" class="pure-img" style="display: block; margin-left: auto; margin-right: auto;" />

<p>Comme vous le voyez les graphes permettent très rapidement de comprendre la structure de la base de données et les relations entre les différents noeuds. Le graphe précédent nous permet facilement de distinguer quelles sont les dépendances directes et indirectes de Scrapy.</p>

<p>Ce graphe ne contient que quelques dizaines de noeuds, nous pouvons donc facilement le parcourir de tête. Mais imaginez un graphe contenant plusieurs centaines de milliers de noeuds liés entre eux par des relations.</p>

<p>Et bien Neo4j répond à cette problématique en nous fournissant les outils nécessaires à la construction et au parcours optimisé de nos graphes !</p>

<h1 id="les-concepts">Les concepts</h1>

<p>Les données stockées dans Neo4j vont donc être organisées sous la forme d’un graphe, les <strong>noeuds</strong> étant reliés entre eux par des <strong>relations</strong>. Des <strong>propriétés</strong> (clé:valeur) peuvent enrichir les noeuds et les relations afin d&rsquo;y ajouter du contexte :</p>

<img src="/images/neo4j_concepts.png" alt="" class="pure-img" style="display: block; margin-left: auto; margin-right: auto;" />

<p>Nous pouvons retrouver 2 noeuds dans ce graphe reliés entre eux par une relation de type <code>WORKS AT</code>. Des propriétés ont été ajoutées afin de compléter les informations propres à chaque noeud.</p>

<p>Des noeuds de même type sont regroupés au sein de <strong>labels</strong> :</p>

<img src="/images/neo4j_concepts_labels.png" alt="" class="pure-img" style="display: block; margin-left: auto; margin-right: auto;" />

<p>Nous avons rajouté un noeud de label <code>Job</code>, lié au noeud <code>Person</code> via une relation de type <code>HAS</code>. Sur ce graphe nous pourrions très facilement répondre à la requête &ldquo;Donnes moi l&rsquo;emploi actuel de tous les développeurs Python Français&rdquo;.</p>

<p>Et c&rsquo;est tout ! Il existe évidemment d&rsquo;autres notions internes à Neo4j, telles que les index ou les contraintes, mais ces 4 notions de base (noeuds, relations, propriétés, labels) suffisent à débuter l&rsquo;utilisation concrète de Neo4j !</p>

<h1 id="installation">Installation</h1>

<p>Neo4j s&rsquo;installe très simplement dès lors que Java 8 est disponible sur votre OS. Je suis pour ma part sur Debian, nous devons donc ajouter le repo officiel à nos sources :</p>

<pre><code>$ wget -O - https://debian.neo4j.org/neotechnology.gpg.key | sudo apt-key add -
$ echo 'deb https://debian.neo4j.org/repo stable/' | sudo tee -a /etc/apt/sources.list.d/neo4j.list
$ sudo apt-get update
</code></pre>

<p>Puis nous installons ensuite Neo4j comme tous les autres packages (la dernière version stable à l&rsquo;écriture de cet article est la <strong>3.5.0</strong>) :</p>

<pre><code>$ sudo apt-get install neo4j=1:3.5.0
</code></pre>

<h1 id="utilisation">Utilisation</h1>

<p>La première chose à faire est d&rsquo;ouvrir le <strong>Neo4j Browser</strong> : il s&rsquo;agit d&rsquo;une application très pratique qui va vous permettre de créer vos données et de les visualiser à travers une interface web.</p>

<p>Le browser est accessible en local sur le port 7474 : <a href="http://localhost:7474">http://localhost:7474</a>.</p>

<p>Vous devez tout d&rsquo;abord vous connecter grâce aux identifiants <strong>neo4j / neo4j</strong> (pas d&rsquo;inquiétude le browser vous demandera tout de suite après de modifier votre mot de passe).</p>

<p>Voici ce que vous devriez obtenir une fois connecté :</p>

<img src="/images/neo4j_browser.png" alt="" style="display: block; margin-left: auto; margin-right: auto; width: 80%;">

<p>Comme vous le voyez le browser peut se découper en 3 parties :</p>

<ol>
<li>C&rsquo;est ici que nous retrouverons des informations sur notre instance Neo4j, comme la version ou la taille, ainsi que la liste des <strong>labels</strong>, des types de <strong>relations</strong> et des <strong>propriétés</strong> stockées dans notre base de données.</li>
<li>Ce formulaire va vous permettre de saisir vos requêtes afin d&rsquo;interroger Neo4j.</li>
<li>Et c&rsquo;est dans cette vue que les résultats de vos précédentes requêtes s&rsquo;afficheront.</li>
</ol>

<p>Nous allons créer les 4 noeuds que j&rsquo;ai donné en exemple dans la partie <em>&ldquo;Les concepts&rdquo;</em>. Pour cela nous allons utilisons un langage de requête créé pour Neo4j : le <strong>Cypher</strong>, qui est un peu le SQL des graphes :)</p>

<p>Par exemple l&rsquo;insertion et la sélection d&rsquo;une données en SQL se ferait grâce à la requête suivante :</p>

<pre><code>CREATE TABLE person (name varchar, from varchar);
INSERT INTO person VALUES ('Nicolas', 'France');
SELECT * FROM person;
</code></pre>

<p>Voici l&rsquo;équivalent en Cypher :</p>

<pre><code>CREATE(nico:Person{name: &quot;Nicolas&quot;, from: &quot;France&quot;}) RETURN nico;
</code></pre>

<p>Vous pouvez remarquer plusieurs choses :</p>

<ul>
<li>la création d&rsquo;un noeud se fait grâce au mot-clé <code>CREATE</code>,</li>
<li>le noeud (que nous avons nommé <code>nico</code>) est entouré de parenthèses,</li>
<li>les propriétés s&rsquo;ajoutent entre les accolades,</li>
<li>nous retournons le noeud nouvellement créé.</li>
</ul>

<p>Notez également que nous n&rsquo;avons pas eu besoin de créer le label <code>Person</code>. En effet Neo4j s&rsquo;est chargé de le faire lui-même au moment de la création du noeud (pour rappel Neo4j fait partie des bases de données NoSQL).</p>

<p>Nous pouvons d&rsquo;ailleurs le voir dans le Browser Neo4j sous la partie <em>&ldquo;Node Labels&rdquo;</em> :</p>

<img src="/images/neo4j_create_node.png" alt="" style="display: block; margin-left: auto; margin-right: auto; width: 80%;">

<p>Nous allons maintenant créer le noeud de type <code>Job</code> que nous lierons avec le noeud <code>Person</code> grâce à la requête suivante :</p>

<pre><code>MATCH(nico:Person{name: &quot;Nicolas&quot;, from: &quot;France&quot;})
CREATE(nico)-[:HAS]-&gt;(job:Job{profile: &quot;Dev&quot;, lang: &quot;Python&quot;})
RETURN nico, job;
</code></pre>

<p>La forme change un peu ici :</p>

<ol>
<li>tout d&rsquo;abord je sélectionne le noeud <code>nico</code>,</li>
<li>puis, en utilisant une seule requête CREATE, je créé en base le noeud <code>job</code> et j&rsquo;ajoute la relation <code>HAS</code> entre les deux,</li>
<li>je retourne pour finir mes 2 noeuds.</li>
</ol>

<p>Les propriétés peuvent être visibles dans le Browser en survolant un noeud :</p>

<img src="/images/neo4j_create_relationship.png" alt="" style="display: block; margin-left: auto; margin-right: auto; width: 80%;">

<p>Pour finir créons en une seule requête les noeuds de type <code>Company</code> :</p>

<pre><code>MATCH(nico:Person{name: &quot;Nicolas&quot;, from: &quot;France&quot;})
MERGE(nico)-[:`WORKS AT`{since: 2015}]-&gt;(ovh:Company{name: &quot;OVH&quot;})
MERGE(nico)-[:`WORKS AT`{until: 2015, since: 2013}]-&gt;(anssi:Company{name: &quot;Anssi&quot;})
RETURN nico, ovh, anssi;
</code></pre>

<p><strong>Note :</strong> Les plus attentifs auront remarqué le mot-clé <code>MERGE</code>, que nous pourrions renommer en <code>GET or CREATE</code>. Il agit ici comme le <code>CREATE</code> que nous avons vu plus tôt, mais cette commande n&rsquo;aurait rien ajouté de plus si les noeuds <code>Company</code> et leurs relations existaient déjà. Je vous la montre car elle peut être très pratique lorsque l&rsquo;on souhaite réaliser des requêtes idempotentes.</p>

<p>Il est désormais temps d&rsquo;afficher notre graphe, et le browser va nous être très pratique car nous allons le faire sans écrire une seule requête :</p>

<ol>
<li>Sélectionnez le label <code>Person</code> dans la liste des labels à gauche de l&rsquo;écran.</li>
<li>Vous devriez voir apparaître le noeud <em>&ldquo;Nicolas&rdquo;</em>, cliquez dessus.</li>
<li>Le noeud s&rsquo;entoure alors de différent bouton, cliquez sur celui du bas afin de <strong>déplier</strong> les dépendances directes.</li>
</ol>

<p>Nous avons bien affiché le graphe complet sans taper une ligne de Cypher :</p>

<img src="/images/neo4j_expand_node.png" alt="" style="display: block; margin-left: auto; margin-right: auto; width: 80%;">

<p>Pour info la requête équivalente aurait été la suivante :</p>

<pre><code>MATCH(n:Person)-[]-&gt;(m) RETURN n, m
</code></pre>

<p>L&rsquo;astuce consiste à sélectionner les noeuds de type <code>Person</code>, puis à retourner l&rsquo;ensemble de ses dépendances sans rien filtrer. Attention néanmoins à cette requête si votre graphe contient trop de données, votre navigateur risque de ne pas apprécier :)</p>

<p>Pour finir cette partie souvenez-vous de la question posée plus haut : &ldquo;Donnes moi l&rsquo;emploi actuel de tous les développeurs Python Français&rdquo;. Et bien la réponse serait apportée par la requête suivante :</p>

<pre><code>MATCH(p:Person{from: 'France'})-[:HAS]-&gt;(:Job{profile: 'Dev', lang: 'Python'})
MATCH(p)-[r:`WORKS AT`]-&gt;(c:Company)
WHERE NOT EXISTS(r.until)
RETURN p.name, c.name
</code></pre>

<img src="/images/neo4j_cypher.png" alt="" style="display: block; margin-left: auto; margin-right: auto; width: 80%;">

<h1 id="conclusion">Conclusion</h1>

<p>Nous n&rsquo;avons fait que survoler Neo4j et le Cypher. Si vous souhaitez en apprendre plus sur ce langage, je vous invite à vous tourner vers la <a href="https://neo4j.com/developer/cypher/">documentation officielle</a>, vous y apprendrez notamment à filtrer vos résultats, à les ordonner ou encore à utiliser des algorithmes optimisés de parcours de graphe.</p>

<p>Comme je le disais en introduction de cet article j&rsquo;utilise désormais Neo4j au quotidien dans le cadre de DEPC. Nous gérons plusieurs millions de noeuds et autant de relations, et je dois dire que je ne suis pas du tout déçu de ce choix technique.</p>

<p>Evidemment le choix d&rsquo;une base de données orientée graphe répondra généralement à une problématique précise. Bien souvent une base de données relationnelle répondra à vos besoins, mais si vos données peuvent s&rsquo;architecturer sous la forme d&rsquo;un graphe alors n&rsquo;hésitez pas et sautez le pas, vous ne serez pas déçu.</p>
                    </div>
                </section>
                
                <h1 class="content-subhead">02 Nov 2018, 09:25</h1>
                <section class="post">
                    <header class="post-header">

                        <a href="https://ncrocfer.github.io/posts/gestion-de-taches-avec-apache-airflow/" class="post-title">Gestion de Tâches avec Apache Airflow</a>

                        <p class="post-meta">
                            
                            
                        </p>
                    </header>

                    <div class="post-description">
                        <p>Apache Airflow est un outil open source d’orchestration de workflows : si vous êtes habitués à gérer des tâches cron au quotidien, alors cet article devrez vous plaire.</p>

<p></p>

<h1 id="airflow-vs-cron">Airflow vs. Cron</h1>

<p>Pour l’histoire Airflow a été créé en 2014 par <a href="https://medium.com/@maximebeauchemin">Maxime Beauchemin </a> lorsqu’il travaillait chez Airbnb. En 2016 le projet a rejoint le programme d’incubation de la fondation Apache, c’est pourquoi nous parlons désormais d’Apache Airflow.</p>

<img src="/images/airflow-logo.png" alt="" style="display: block; margin-left: auto; margin-right: auto; width: 50%;">

<p>Alors qu’est-ce qui le différencie des tâches cron ? Après tout nous y sommes habitués, et il faut avouer qu’il est très simple de lancer de manière régulière un script, une commande ou un programme.</p>

<p>Mais il y a fort à parier que votre tâche rencontrera tôt ou tard un problème :</p>

<ul>
<li>un timeout sur une requête SQL,</li>
<li>un changement de clé dans le Json d’une API que vous interrogez,</li>
<li>un site web injoignable au moment d’un GET,</li>
<li>etc.</li>
</ul>

<p>A la limite si notre script est très simple nous pouvons gérer ces cas dans le code lui-même, il ne faudra en oublier aucun, mais ça reste faisable.</p>

<p>En revanche les choses se compliquent lorsque nous devons gérer plusieurs tâches dépendantes les unes des autres. Imaginez qu’une tâche B ait besoin de données créées par une tâche A, elle doit donc s’exécuter après. En cron nous laisserions un temps raisonnable entre le lancement des 2 tâches pour que celles-ci s’exécutent correctement. Sauf que ce laps de temps est fixé, si la tâche A plante pour une raison ou une autre, la tâche B s’exécutera tout de même avec potentiellement des données corrompues.</p>

<p>C’est là qu’Airflow entre en jeu : cet outil va justement nous permettre de gérer les dépendances entre nos tâches, on parle alors de <strong>workflow</strong> ou encore de <strong>pipeline</strong>. Nous déclarons nos tâches à Airflow, et ce dernier s’occupe de les lancer dans le bon ordre, de gérer les exceptions s’il y en a, et de relancer les tâches le cas échéant.</p>

<p>En bonus Airflow est livré avec une Webui extrêmement pratique qui va nous permettre de suivre l’avancement de nos tâches et de les monitorer :</p>

<img src="/images/airflow.gif" alt="" style="display: block; margin-left: auto; margin-right: auto; width: 75%;">

<h1 id="les-concepts">Les Concepts</h1>

<p>L&rsquo;utilisation d&rsquo;Airflow se fait via du code Python. Mais avant cela voici quelques concepts qu&rsquo;il vous faudra connaître afin de mettre les mains dans le cambouis.</p>

<h2 id="dag-tasks">DAG &amp; Tasks</h2>

<p>Airflow est principalement basé sur le concept de <strong>DAG</strong>, pour <a href="https://en.wikipedia.org/wiki/Directed_acyclic_graph">Directed Acyclic Graph</a>. Un DAG n’est ni plus ni moins qu’un graphe orienté sans retour possible. Nos tâches s’exécuteront donc dans un ordre précis, en parallèle ou à la suite, et ce sans risque de boucle infinie.</p>

<p><img https://en.wikipedia.org/wiki/Directed_acyclic_graph /></p>

<p>Un DAG est constitué de <strong>tasks</strong> liées les unes aux autres. Vous avez déjà certainement entendu parlé de l’acronyme ETL pour Extract Transform Load, qui consiste à récupérer des données d’une source, à les transformer dans un nouveau format, puis à les renvoyer dans une seconde source adaptée à ce nouveau format.</p>

<p>Et bien Airflow s’adapte parfaitement bien à ce modèle. Voici un exemple de DAG contenant 3 tâches :</p>

<img src="/images/airflow-etl.png" alt="" style="display: block; margin-left: auto; margin-right: auto; ">

<p>Un DAG est lancé de manière régulière ou via un trigger. Un DAG lancé ou en cours d’exécution est appelé une <strong>DagInstance</strong>, ses tâches étant donc des <strong>TaskInstances</strong>.</p>

<h2 id="operators">Operators</h2>

<p>Les tâches d’un DAG ne sont pas toutes forcément du même type : nous aurons parfois besoin de lancer un script Bash, d’autres fois notre logique métier sera contenue dans une fonction Python, et parfois nous aurons simplement besoin d’envoyer un mail.</p>

<p>La création d’une tâche passe par des <strong>operators</strong>. A chaque besoin son operator :</p>

<ul>
<li><code>BashOperator</code> : exécute une commande bash,</li>
<li><code>PythonOperator</code> : exécute une fonction Python,</li>
<li><code>EmailOperator</code> : envoie un mail,</li>
<li><code>DockerOperator</code> : exécute une commande dans un container Docker,</li>
<li><code>HttpOperator</code> : effectue une requête sur un endpoint HTTP,</li>
<li>etc…</li>
</ul>

<p>La liste des operators intégrés à Airflow est <a href="https://github.com/apache/incubator-airflow/tree/master/airflow/operators">longue</a>, et vous pouvez bien sûr créer les vôtres si besoin.</p>

<p>Pour l&rsquo;aspect technique, un operator est tout simplement une classe Python héritant de <code>BaseOperator</code>. Lorsque la tâche est appelée, la fonction <code>execute()</code> de l’operator est exécutée. Vous l’aurez compris, un operator instancié devient donc une task.</p>

<h2 id="executors">Executors</h2>

<p>Lorsqu’une tâche est appelée, elle va s’exécuter <em>quelque part</em> : ce <em>quelque part</em> est géré par Airflow via les <strong>executors</strong>.</p>

<p>Par défaut Airflow utilise le <strong>SequentialExecutor</strong> : les tâches sont lancées les unes après les autres sur le serveur local (là où Airflow lui-même est lancé) sans aucun parallélisme. Cet executor est pratique lorsqu’on développe, mais les limites sont vites atteintes lorsque l’on passe à l’échelle.</p>

<p>On pourra dans ce cas utiliser le <strong>LocalExecutor</strong>, plus performant que le précédent car basé sur un modèle prefork : plusieurs tâches pourront donc être lancées en parallèle jusqu’à ce que les ressources de la machine locale soient utilisées. Ici le scaling est vertical (plus de ressources signifie plus de tasks lancées en même temps).</p>

<p>Et le meilleur pour la fin : le <strong>CeleryExecutor</strong>. C’est via cet executor que le scaling des tâches pourra réellement se faire puisqu’Airflow ira déléguer à Celery la distribution des tâches sur un pool de workers. Le scaling sera donc horizontal : plus nous aurons de workers plus nous pourrons lancer de tâches en parallèle. Notons tout de même que Celery nécessite l’utilisation d’un broker pour l’échange des messages, ainsi qu’une synchronisation du code sur l’ensemble des workers (facilement réalisable avec l’utilisation de Redis &amp; Kubernetes par exemple).</p>

<h1 id="création-du-dag">Création du DAG</h1>

<p>Bon assez parlé, je me doute que vous avez envie de tester Airflow de ce pas. L’installation est commune à tous les packages Python :</p>

<pre><code class="language-bash">$ pip install apache-airflow
</code></pre>

<p>Airflow va chercher les Dags dans le répertoire <strong>~/airflow</strong>, mais nous pouvons changer ce comportement via la variable d’environnement <code>AIRFLOW_HOME</code> :</p>

<pre><code class="language-bash">$ export AIRFLOW_HOME=~/Dev/python/demo_airflow
</code></pre>

<p>Une base de données est nécessaire pour faire fonctionner Airflow, nous la créons avec la CLI fournie lors de l’installation :</p>

<pre><code class="language-bash">$ airflow initdb
$ ls
airflow.cfg   airflow.db    logs          unittests.cfg
</code></pre>

<p>Plusieurs fichiers ont été créés, les plus importans étant les deux suivants :</p>

<ul>
<li><code>airflow.db</code> : il s&rsquo;agit de la base de données utilisées par Airflow pour gérer nos Dags et leurs tasks. Ce fichier est au format SQLite3, on pourra le garder tout au long de nos développements mais je vous conseille fortement de passer sur un autre type (PostgreSQL ou MySQL par exemple) lors du passage en production.</li>
<li><code>airflow.cfg</code> : ce fichier contient la configuration d&rsquo;Airflow, c&rsquo;est ici par exemple que vous configurerez l&rsquo;accès à la base de données, l&rsquo;executor à utiliser, les paramètres de parallélisation, et j&rsquo;en passe.</li>
</ul>

<p>Créons maintenant notre DAG qui contiendra plus tard nos différentes tasks. La première étape consiste à instancier un object DAG et à lui fournir certains paramètres :</p>

<pre><code class="language-python">from datetime import datetime, timedelta

from airflow import DAG

default_args = {
    'owner': 'ncrocfer',
    'start_date': datetime(2018, 10, 31),
    'retries': 5,
    'retry_delay': timedelta(minutes=5)
}

dag = DAG('demo', default_args=default_args, schedule_interval='0 * * * *')
</code></pre>

<p>Les paramètres que nous déclarons dans la variable <code>default_args</code> sont assez parlants :</p>

<ul>
<li><code>owner</code> : l&rsquo;utilisateur du DAG.</li>
<li><code>start_date</code> : Airflow lancera le DAG à partir de cette date.</li>
<li><code>retries</code> : le nombre d&rsquo;essai qui sera effectué si une tâche tombe en erreur. Une fois cette valeur atteinte, le DAG tombera en erreur et sera stoppé.</li>
<li><code>retry_delay</code> : le temps d&rsquo;attente entre deux essais.</li>
</ul>

<p>Vous avez dû remarquer un pattern qui vous parle si vous utilisez les tâches cron : <code>0 * * * *</code>. C&rsquo;est en effet ici que l&rsquo;on définit le cycle d&rsquo;exécution du DAG, dans notre cas notre DAG <code>demo</code> sera exécuté <strong>chaque heure</strong>. La syntaxe est la même qu&rsquo;avec les crontab, et certains raccourcis sont <a href="https://airflow.apache.org/scheduler.html#dag-runs">disponibles</a>.</p>

<p>Vous devriez déjà voir votre DAG apparaître dans la liste :</p>

<pre><code class="language-bash">$ airflow list_dags
[2018-11-01 16:35:35,220] {__init__.py:51} INFO - Using executor SequentialExecutor
[2018-11-01 16:35:35,564] {models.py:258} INFO - Filling up the DagBag from /Users/ncrocfer/Dev/python/demo_airflow/dags


-------------------------------------------------------------------
DAGS
-------------------------------------------------------------------
demo
</code></pre>

<p><strong>Note :</strong> il se peut que d&rsquo;autres DAG apparaissent dans votre liste. C&rsquo;est en fait Airflow qui inclut des exemples de DAG. Vous pouvez les supprimer en passant à <code>False</code> la variable <code>load_examples</code> du fichier <code>airflow.cfg</code>, puis en remettant à zéro la base de données avec la commande <code>airflow resetdb</code>.</p>

<h1 id="création-des-tâches">Création des Tâches</h1>

<p>Nous devons maintenant créer nos tâches. Je vais prendre comme exemple le site <a href="https://www.saucs.com">Saucs.com</a>, pour lequel je gère la partie développement. Pour rappel ce site permet aux utilisateurs de s&rsquo;abonner à des produits et de recevoir un rapport par mail dès qu&rsquo;une vulnérabilité apparaît sur le site du NVD.</p>

<p>L&rsquo;ordre des tâches est donc le suivant :</p>

<ol>
<li><code>CheckUpdates</code> : On vérifie sur le site du NVD si une mise à jour des CVE a eu lieu,</li>
<li><code>DownloadXml</code> : On télécharge les nouvelles CVE,</li>
<li><code>ParseXml</code> : On parse les nouvelles CVE et on met à jour notre base de données,</li>
<li><code>SendMails</code> : On envoie un mail aux utilisateurs abonnés aux produits impactés.</li>
</ol>

<p>Comme expliqué plus haut, la création de tâches se fait via l’utilisation des operators. Nous allons pour cet exemple utiliser plusieurs <code>PythonOperator</code> auxquels nous passons des fonctions Python :</p>

<p>Voici le code final de notre DAG :</p>

<pre><code class="language-python">import gzip
import logging
import re
from datetime import datetime, timedelta
from io import BytesIO
from xml.etree import ElementTree as ET

import requests
from airflow import DAG
from airflow.operators.python_operator import BranchPythonOperator, PythonOperator


default_args = {
    'owner': 'ncrocfer',
    'start_date': datetime(2018, 10, 31),
    'retries': 5,
    'retry_delay': timedelta(minutes=5)
}


logger = logging.getLogger(__name__)


NVD_MODIFIED_META_URL = 'https://static.nvd.nist.gov/feeds/xml/cve/2.0/nvdcve-2.0-Modified.meta'
NVD_MODIFIED_URL = 'https://static.nvd.nist.gov/feeds/xml/cve/nvdcve-2.0-Modified.xml.gz'
LAST_NVD_HASH = '/tmp/lastnvd'


def check_updates():
    logger.info(&quot;Downloading {url}...&quot;.format(url=NVD_MODIFIED_META_URL))

    resp = requests.get(NVD_MODIFIED_META_URL)
    buf = BytesIO(resp.content).read().decode('utf-8')

    nvd_sha256 = buf.split('sha256')[1][1:-2]
    logger.info(&quot;New NVD hash is : {hash}&quot;.format(hash=nvd_sha256))

    try:
        with open(LAST_NVD_HASH) as f:
            last_nvd256 = f.read()
    except FileNotFoundError:
        last_nvd256 = None
    logger.info(&quot;Local hash is : {hash}&quot;.format(hash=last_nvd256))

    if nvd_sha256 != last_nvd256:
        logger.info(&quot;Hashes differ, Saucs database needs to be updated...&quot;)
        return {'updated': True, 'hash': nvd_sha256}
    else:
        logger.info(&quot;Hashes are the same, nothing to do.&quot;)
        return {'updated': False}


def download_xml(ds, **context):
    update = context['task_instance'].xcom_pull(task_ids='CheckUpdates')
    if not update['updated']:
        return

    logger.info(&quot;Downloading {url}...&quot;.format(url=NVD_MODIFIED_URL))

    resp = requests.get(NVD_MODIFIED_URL)
    buf = BytesIO(resp.content)
    data = gzip.GzipFile(fileobj=buf)

    xml_string = data.read().decode('utf-8')
    xml_string = re.sub(' xmlns=&quot;[^&quot;]+&quot;', '', xml_string)

    with open('/tmp/{0}.xml'.format(ds), 'w') as f:
        f.write(xml_string)


def parse_xml(ds, **context):
    update = context['task_instance'].xcom_pull(task_ids='CheckUpdates')
    if not update['updated']:
        return

    parser = ET.XMLParser(encoding=&quot;utf-8&quot;)
    tree = ET.parse('/tmp/{0}.xml'.format(ds), parser=parser)
    root = tree.getroot()

    for child in root:
        logger.info(&quot;Parsing {cve}...&quot;.format(cve=child.attrib.get('id')))

        # 1. Process the CVE (new CVE, CPE updated, references updated, CVSS updated...)
        # 2. Create an alert for all impacted users


def send_mails(ds, **context):
    update = context['task_instance'].xcom_pull(task_ids='CheckUpdates')
    if not update['updated']:
        return

    logger.info('Sending mail for users with an alert')
    &quot;&quot;&quot;users = get_users_with_alerts()

    for user in users:
        send_user_mail()
        user.new_alerts = False&quot;&quot;&quot;

    # We're done, we can set the new NVD hash in local
    with open(LAST_NVD_HASH, 'w') as f:
        f.write(update['hash'])


dag = DAG('demo', default_args=default_args, schedule_interval='0 * * * *')

with dag:
    check_updates_op = PythonOperator(
        task_id='CheckUpdates',
        python_callable=check_updates
    )

    download_xml_op = PythonOperator(
        task_id='DownloadXml',
        provide_context=True,
        python_callable=download_xml
    )

    parse_xml_op = PythonOperator(
        task_id='ParseXml',
        provide_context=True,
        python_callable=parse_xml
    )

    send_mails_op = PythonOperator(
        task_id='SendMails',
        provide_context=True,
        python_callable=send_mails
    )


# Order the tasks
check_updates_op &gt;&gt; download_xml_op &gt;&gt; parse_xml_op &gt;&gt; send_mails_op

&quot;&quot;&quot;
# Can be written :
check_updates_op.set_downstream(download_xml_op)
download_xml_op.set_downstream(parse_xml_op)
parse_xml_op.set_downstream(send_mails_op)
&quot;&quot;&quot;
</code></pre>

<p>La logique des 4 fonctions n&rsquo;est pas compliquée et ne concerne pas directement Airflow, je ne m&rsquo;étalerai pas dessus. En revanche certaines parties du code sont nouvelles :</p>

<ol>
<li>On constate l&rsquo;ouverture d&rsquo;un context manager (<code>with dag</code>) juste après la déclaration du DAG : cela signifie que tous les opérateurs déclarés à l&rsquo;intérieur de celui-ci seront liés à notre DAG.</li>
<li>Certains opérateurs possèdent le paramètre <code>provide_context</code> : dans ce cas les fonctions à lancer (renseignées via le paramètre <code>python_callable</code>) recevront les paramètres <code>ds</code> et <code>context</code>. Le premier est un string représentant la date d&rsquo;éxecution de la tâche, le second est un dictionnaire contenant des informations relatives à l&rsquo;instance de tâche.</li>
<li>Nous utilisons les <a href="https://airflow.apache.org/concepts.html#xcoms">xcom</a> : il s&rsquo;agit d&rsquo;une fonctionnalité proposée par Airflow afin de communiquer des informations d&rsquo;une tâche à une autre.</li>
<li>la dernière ligne peut paraître obscure : il s&rsquo;agit en fait tout simplement de la façon dont nous déclarons l&rsquo;ordre de lancement de nos tâches. Nous utilisons ici les opérateurs bits à bits (<a href="https://wiki.python.org/moin/BitwiseOperators">Bitwise operators</a>) car je trouve que la forme est assez parlante, mais Airflow propose également les méthodes <code>set_upstream()</code> et <code>set_downstream()</code>.</li>
</ol>

<p>Nous pouvons tester que tout fonctionne correctement grâce à la commande <code>list_tasks</code> :</p>

<pre><code class="language-bash">$ airflow list_tasks demo --tree
2018-11-01 18:45:03,572] {__init__.py:51} INFO - Using executor SequentialExecutor
[2018-11-01 18:45:03,785] {models.py:258} INFO - Filling up the DagBag from /Users/ncrocfer/Dev/python/demo_airflow/dags
&lt;Task(PythonOperator): SendMails&gt;
    &lt;Task(PythonOperator): ParseXml&gt;
        &lt;Task(PythonOperator): DownloadXml&gt;
            &lt;Task(PythonOperator): CheckUpdates&gt;
</code></pre>

<h1 id="exécution-du-dag">Exécution du DAG</h1>

<p>Tout semble en ordre, nous pouvons lancer le serveur web :</p>

<pre><code class="language-bash">$ airflow webserver
</code></pre>

<p>Par défaut l&rsquo;interface est disponible en localhost sur le port 8080. Nous pouvons y voir notre DAG :</p>

<img src="/images/airflow-webui.png" alt="" style="display: block; margin-left: auto; margin-right: auto; width: 100%;">

<p>L&rsquo;exécution du DAG nécessite deux actions :</p>

<ul>
<li>tout d&rsquo;abord il faut l&rsquo;activer (le bouton On/Off),</li>
<li>puis vous devez lancer le <strong>scheduler</strong> d&rsquo;Airflow.</li>
</ul>

<p>C&rsquo;est ce dernier qui va gérer pour nous l&rsquo;ordonnancement des tâches, leur retry si besoin, et c&rsquo;est également lui qui vérifiera en permanence si un nouveau DagRun ne doit pas être lancé :</p>

<pre><code class="language-bash">$ airflow scheduler
</code></pre>

<p>Cliquez maintenant sur le DAG <strong>demo</strong>, puis sur l&rsquo;onglet <strong>Graph View</strong>. Le DAG a bien été lancé :</p>

<img src="/images/airflow-launched.png" alt="" style="display: block; margin-left: auto; margin-right: auto; width: 100%;">

<p>Airflow s&rsquo;occupe désormais de tout et va lancer le DAG chaque heure, et ce depuis le <code>start_date</code> (le scheduler effectue ce qu&rsquo;on appelle le <strong>backfill</strong>, ce comportement étant modifiable grâce à la configuration <code>catchup_by_default</code> ou au paramètre <code>catchup</code> du DAG lui-même) :</p>

<img src="/images/airflow-backfill.png" alt="" style="display: block; margin-left: auto; margin-right: auto; width: 100%;">

<p>Pour finir il est important de noter que le monitoring de nos tâches est très simple puisqu&rsquo;Airflow fournit une interface pour visualiser les logs (et donc potentiellement les exceptions lancées).</p>

<p>Vous pouvez voir les logs d&rsquo;une tâche en cliquant sur la tâche puis sur le bouton <strong>View Log</strong> :</p>

<img src="/images/airflow-logs.png" alt="" style="display: block; margin-left: auto; margin-right: auto; width: 100%;">

<h1 id="conclusion">Conclusion</h1>

<p>Je m&rsquo;arrête ici pour cette présentation d&rsquo;Apache Airflow. Il y a beaucoup de chose à dire dessus, nous n&rsquo;avons fait qu&rsquo;effleurer le sujet mais j&rsquo;espère néanmoins vous avoir donner envie de le tester.</p>

<p>L&rsquo;exemple que j&rsquo;ai donné était très simple : j&rsquo;ai volontairement épuré le Dag de Saucs afin de vous montrer un exemple concret d&rsquo;usage d&rsquo;Airflow sans polluer le code par de la logique externe au tool. De même vous aurez peut-être remarqué que chaque tâche est exécutée même si aucune mise à jour n&rsquo;a été effectuée par le NVD. La solution dans ce cas aurait été d&rsquo;utiliser l&rsquo;opérateur <code>BranchPythonOperator</code> afin de renvoyer vers la tâche adéquate, mais cela aurait compliqué cet article et ce n&rsquo;était pas l&rsquo;objectif souhaité.</p>

<p>Airflow est un outil que j&rsquo;utilise désormais au quotidien, et je ne peux que vous conseiller de l&rsquo;essayer vous-aussi. Je tenterai dans les prochains articles de couvrir d&rsquo;autres fonctionnalités d&rsquo;Airflow, tels que les BranchOperator, le CeleryExecutor ou encore la création dynamique de Dags.</p>
                    </div>
                </section>
                
                <h1 class="content-subhead">07 Aug 2017, 01:03</h1>
                <section class="post">
                    <header class="post-header">

                        <a href="https://ncrocfer.github.io/posts/gerer-vos-cve-avec-saucs/" class="post-title">Gérer vos CVE avec Saucs</a>

                        <p class="post-meta">
                            
                            
                        </p>
                    </header>

                    <div class="post-description">
                        <p>Je vais vous présenter un projet infosec sur lequel je travaille depuis plusieurs mois : <a href="https://www.saucs.com">Saucs.com</a>.</p>

<p>Cette plateforme web vous permet de manager vos alertes de sécurité en vous abonnant à différents produits et vendeurs. Dès lors qu&rsquo;une nouvelle vulnérabilité est détectée, une alerte est créée et un rapport vous est envoyé par mail.</p>

<p></p>

<p>Vous pouvez également <a href="https://www.saucs.com/cve">rechercher les vulnérabilités</a> par produit ou plus globalement par vendeur :</p>

<img src="/images/saucs_cves.png" alt="" class="pure-img" style="display: block; margin-left: auto; margin-right: auto;" />

<p>Saucs se repose sur les bases maintenues par le <a href="https://cve.mitre.org/">MITRE</a> et le <a href="https://nvd.nist.gov/vuln/full-listing">NVD</a> pour se synchroniser :</p>

<ul>
<li><strong>CVE</strong> (Common Vulnerability Enumeration),</li>
<li><strong>CPE</strong> (Common Platform Enumeration),</li>
<li><strong>CWE</strong> (Common Weakness Enumeration),</li>
<li><strong>CVSS</strong> (Common Vulnerability Scoring System).</li>
</ul>

<p>Ces bases contiennent des données organisées permettant de classifier les vulnérabilités relatives à des produits. Saucs utilise par exemple les CPE afin de fournir une liste de vendeurs et de produits auxquels s&rsquo;abonner :</p>

<img src="/images/saucs_vendors_products.png" alt="" class="pure-img" style="display: block; margin-left: auto; margin-right: auto;" />

<p>Concrètement les <a href="http://www.google.com">données fournies</a> par le NVD sont parsées régulièrement et des tâches Celery analysent les abonnements des utilisateurs pour envoyer les notifications. Il peut s&rsquo;agir d&rsquo;une nouvelle CVE, mais également de la modification d&rsquo;une CVE existante : nouvelle référence, changement du score CVSS, etc.</p>

<img src="/images/saucs_email.png" alt="" class="pure-img" style="display: block; margin-left: auto; margin-right: auto;" />

<p>J&rsquo;ai débuté ce projet seul : c&rsquo;est une idée que j&rsquo;avais en tête depuis pas mal de temps, et en tant que dev Python aucun problème pour créer le site et les robots. Mais ceux qui administrent leur propre site le savent, un site web ce n&rsquo;est pas que du dev :)</p>

<p>J&rsquo;ai donc rapidement été rejoint par <a href="https://twitter.com/LaurentDurnez">Laurent</a> pour prendre en charge la partie administration : il gère entre autres les serveurs web, les mails, le load balancing, le scaling automatique, l&rsquo;architecture master/slave des databases, leur réplication, etc. Il fera d&rsquo;ailleurs prochainement un article sur le blog de Saucs afin d&rsquo;expliquer comment bootstraper rapidement une architecture HA et fault tolerant.</p>

<p>Côté dev le site est entièrement codé en Python, grâce au framework Flask. Les tâches asynchrones (comme le parsing des CVE, la création des reports, l&rsquo;envoie de mail, la mise en cache) sont prises en charge par Celery.</p>

<p>Je ne rentre pas plus en détail pour le moment sur ces aspects techniques puisque des articles dédiés seront mis en ligne sur le <a href="https://blog.saucs.com">blog officiel</a> de Saucs. Nous aimerions décrire les changements qui interviendront sur l&rsquo;architecture de la plateforme au fur et à mesure que celle-ci grandira.</p>

<p>En parlant de ça, nous avons lancé le site il y a 3 semaines pour tâter le terrain et savoir si ce genre d&rsquo;outil pouvait être utile à d&rsquo;autres personne. Une semaine après l&rsquo;envoi d&rsquo;un premier <a href="https://twitter.com/ncrocfer/status/884711385686577152">Tweet</a> nous étions déjà à 500 utilisateurs! Nous avons reçus beaucoup de feedbacks, notamment des demandes sur l&rsquo;ajout d&rsquo;autres sources ainsi qu&rsquo;un système de filtres d&rsquo;alertes.</p>

<p>C&rsquo;est donc ce que nous allons faire dans les semaines à venir, à commencer par le système de <strong>Report</strong> qui a été lancé ce week-end :</p>

<img src="/images/saucs_report.png" alt="" class="pure-img" style="display: block; margin-left: auto; margin-right: auto;" />

<p>Au niveau des fonctionnalités à venir, nous avons énormément d&rsquo;idées :</p>

<ul>
<li>filtrer les types de notifications,</li>
<li>ajout de nouvelles sources (DSA par exemple),</li>
<li>intégration de sources externes pour les notifications (Slack, Mattermost&hellip;),</li>
<li>discussions au sein de chaque CVE,</li>
<li>API / SDK,</li>
<li>applications mobiles,</li>
<li>etc.</li>
</ul>

<p>Bref comme vous le voyez nous avons pas mal d&rsquo;idées, et nous pensons que Saucs peut avoir un réel intérêt pour les particuliers comme pour les entreprises.</p>

<p>N&rsquo;hésitez pas à nous contacter sur Twitter (<a href="https://twitter.com/LaurentDurnez">Laurent</a> ou <a href="https://twitter.com/ncrocfer">moi</a>) , tout feedback est le bienvenu :)</p>
                    </div>
                </section>
                
                <h1 class="content-subhead">23 Dec 2015, 23:22</h1>
                <section class="post">
                    <header class="post-header">

                        <a href="https://ncrocfer.github.io/posts/installer-un-module-a-la-creation-dun-virtualenv/" class="post-title">Installer un module à la création d&#39;un virtualenv</a>

                        <p class="post-meta">
                            
                            
                        </p>
                    </header>

                    <div class="post-description">
                        <p>L&rsquo;utilisation des environnements virtuels (avec <strong>Virtualenv</strong> et <strong>Virtualenvwrapper</strong>) est inévitable si vous développez régulièrement en Python. D&rsquo;une part leur utilisation permet de laisser votre hôte propre, et d&rsquo;autre part différentes versions d&rsquo;un même module peuvent être testées de manière très simple.</p>

<p></p>

<p>En revanche aucun module n&rsquo;est disponible par défaut dans un nouvel environnement récemment créé.</p>

<p>Pour cela 2 solutions existent : la première est d&rsquo;importer les modules disponibles dans l&rsquo;environnement global (votre host). En gros un <strong>pip freeze</strong> dans votre host et dans votre virtualenv renverra le même résultat :</p>

<pre><code class="language-bash">$ mkvirtualenv foo --system-site-packages
</code></pre>

<p>L&rsquo;idée est d&rsquo;installer et de mettre à jour sur votre host les modules que vous utilisez le plus souvent, et d&rsquo;installer dans le virtualenv ceux qui sont plus spécifiques à chaque projet.</p>

<p>La seconde solution, si vous ne souhaitez vraiment rien installer sur le host, et d&rsquo;installer automatiquement les modules à la création du virtualenv. C&rsquo;est dans le fichier <strong>~/.virtualenvs/postmkvirtualenv</strong> que les commandes se font :</p>

<pre><code class="language-bash">#!/bin/bash
# This hook is sourced after a new virtualenv is activated.

pip install requests
pip install httpie
</code></pre>

<p>La création de l&rsquo;environnement est un peu plus longue mais les modules sont directement disponibles dans leur dernière version :</p>

<pre><code class="language-bash">$ mkvirtualenv foo
New python executable in foo/bin/python
Installing setuptools, pip...done.
Downloading/unpacking requests
  Downloading requests-2.9.1-py2.py3-none-any.whl (501kB): 501kB downloaded
Installing collected packages: requests
Successfully installed requests
Cleaning up...
Downloading/unpacking httpie
  Downloading httpie-0.9.2-py2.py3-none-any.whl (66kB): 66kB downloaded
Requirement already satisfied (use --upgrade to upgrade): requests&gt;=2.3.0 in /home/ncrocfer/.virtualenvs/foo/lib/python2.7/site-packages (from httpie)
Downloading/unpacking Pygments&gt;=1.5 (from httpie)
  Downloading Pygments-2.0.2-py2-none-any.whl (672kB): 672kB downloaded
Installing collected packages: httpie, Pygments
Successfully installed httpie Pygments
Cleaning up...
(foo) $ pip freeze
Pygments==2.0.2
argparse==1.2.1
httpie==0.9.2
requests==2.9.1
wsgiref==0.1.2
(foo) $
</code></pre>
                    </div>
                </section>
                
                <h1 class="content-subhead">08 Dec 2015, 22:58</h1>
                <section class="post">
                    <header class="post-header">

                        <a href="https://ncrocfer.github.io/posts/twitter-calendar/" class="post-title">Twitter calendar</a>

                        <p class="post-meta">
                            
                            
                        </p>
                    </header>

                    <div class="post-description">
                        <p>Si vous utilisez de temps à autre Github, vous êtes déjà probablement tombé sur le profil d&rsquo;un développeur avec le calendrier de ses contributions. Plus le nombre de contributions est élevé, plus la couleur de la case sera foncée.</p>

<p></p>

<p>Je suis tombé sur la librairie <a href="https://github.com/wa0x6e/cal-heatmap">cal-heatmap</a> qui permet de faire exactement la même chose avec les données que l&rsquo;on souhaite. J&rsquo;ai donc décidé de créer l&rsquo;équivalent pour Twitter, afin de visualiser facilement le <strong>nombre de Tweets envoyés par jour</strong> pour un utilisateur donné :</p>

<img src="/images/twitter-calendar.png" alt="" class="pure-img" style="display: block; margin-left: auto; margin-right: auto;" />

<p>Ce calendrier représente les Tweets envoyés sur mon <a href="https://twitter.com/ncrocfer">compte Twitter</a>. Les données datent un peu puisque j&rsquo;ai poussé le code sur Github il y a déjà quelques mois (et j&rsquo;ai la flemme de refaire un screenshot ^^).</p>

<p>Les sources sont disponibles sur Github à <a href="https://github.com/ncrocfer/twitter-calendar">cette adresse</a>.</p>

<p>Alors ok, ce n&rsquo;est pas super utile, mais je trouve plutôt sympa d&rsquo;avoir une représentation visuelle de son activité quotidienne sur Twitter. Et c&rsquo;est un bon exemple de ce qu&rsquo;il est possible de faire avec ce module javascript.</p>
                    </div>
                </section>
                
                <h1 class="content-subhead">30 Nov 2015, 21:01</h1>
                <section class="post">
                    <header class="post-header">

                        <a href="https://ncrocfer.github.io/posts/autoreload-des-modules-sous-ipython/" class="post-title">Autoreload des modules sous iPython</a>

                        <p class="post-meta">
                            
                            
                        </p>
                    </header>

                    <div class="post-description">
                        <p>Si vous utilisez iPython comme interpréteur Python (et si ce n&rsquo;est pas le cas vous loupez quelque chose), il vous est certainement déjà arrivé de bosser dessus dans un terminal, dans un autre de modifier l&rsquo;un des modules importés, puis de relancer la fonction dans iPython. Sauf que ce dernier n&rsquo;a pas rechargé le code et que le résultat est le même qu&rsquo;avant.</p>

<p></p>

<p>Prenez par exemple le code suivant :</p>

<pre><code class="language-python">def calcul(a, b):
    print(a+b)
</code></pre>

<p>Bon ok c&rsquo;est bidon, mais c&rsquo;est pour montrer le principe. Dans un autre terminal, démarrez iPython puis testez-le :</p>

<pre><code class="language-python">In [1]: from calcul import calcul
In [2]: calcul(2, 3)
5
</code></pre>

<p>Si vous modifiez la fonction dans le second terminal, par exemple :</p>

<pre><code class="language-python">def calcul(a, b):
    print(a*b)
</code></pre>

<p>La mise à jour ne sera pas répercutée dans iPython, il faudrait le fermer puis le relancer. L&rsquo;activation de l&rsquo;<strong>autoreload</strong> permet d&rsquo;éviter cela :</p>

<pre><code class="language-ipython">In [3]: calcul(2, 3)
5

In [4]: %load_ext autoreload

In [5]: %autoreload 2

# ré-enregistrez le module calcul.py

In [6]: calcul(2, 3)
6
</code></pre>

<p>Comme vous le voyez les modifications effectuées après l&rsquo;activation de l&rsquo;extension sont automatiquement pris en compte. Si vous souhaitez que l&rsquo;extension soit activée à chaque session, il faut créer un nouveau profile :</p>

<pre><code class="language-bash">ncrocfer@home:~/ ipython profile create
[ProfileCreate] Generating default config file: '/home/ncrocfer/.ipython/profile_default/ipython_config.py'
</code></pre>

<p>Puis ajoutez à la fin les lignes suivantes :</p>

<pre><code class="language-bash">c.InteractiveShellApp.exec_lines = []
c.InteractiveShellApp.exec_lines.append('%load_ext autoreload')
c.InteractiveShellApp.exec_lines.append('%autoreload 2')
</code></pre>

<p>L&rsquo;autoreload des modules importés s&rsquo;effectuera désormais automatiquement !</p>
                    </div>
                </section>
                
                <h1 class="content-subhead">27 Nov 2015, 00:10</h1>
                <section class="post">
                    <header class="post-header">

                        <a href="https://ncrocfer.github.io/posts/installer-python-3-5-1rc1/" class="post-title">Installer Python 3 5 1rc1</a>

                        <p class="post-meta">
                            
                            
                        </p>
                    </header>

                    <div class="post-description">
                        <p>La version 3.5.1rc1 de Python est sortie cette semaine, incluant <a href="https://docs.python.org/3.5/whatsnew/changelog.html#python-3-5-1-release-candidate-1">de nombreuses corrections</a>.</p>

<p>Voici comment l&rsquo;installer si vous souhaitez la tester. Sur mon laptop j&rsquo;ai un répertoire dans lequel je place les sources des différentes versions de Python (<strong>~/dev/python/src</strong>), et un autre dans lequel je compile les binaires (<strong>~/dev/python/bin</strong>).</p>

<p></p>

<p>On commence par télécharger les sources :</p>

<pre><code class="language-bash">ncrocfer@home:~$ cd ~/dev/python/src
ncrocfer@home:~/dev/python/src$ wget https://www.python.org/ftp/python/3.5.1/Python-3.5.1rc1.tgz
ncrocfer@home:~/dev/python/src$ tar xzvf Python-3.5.1rc1.tgz
ncrocfer@home:~/dev/python/src$ cd Python-3.5.1rc1/
</code></pre>

<p>On compile ensuite les sources, en choisissant le répertoire de destination souhaité :</p>

<pre><code class="language-bash">ncrocfer@home:~/dev/python/src/Python-3.5.1rc1$ ./configure --prefix=&quot;/home/ncrocfer/dev/python/bin/3.5.1rc1/&quot;
ncrocfer@home:~/dev/python/src/Python-3.5.1rc1$ make
ncrocfer@home:~/dev/python/src/Python-3.5.1rc1$ make install
</code></pre>

<p>Vous pouvez désormais tester Python 3.5.1rc1 :</p>

<pre><code class="language-bash">ncrocfer@home:~$ cd ~/dev/python/bin/3.5.1rc1/bin/
ncrocfer@home:~/dev/python/bin/3.5.1rc1/bin$ ./python3
Python 3.5.1rc1 (default, Nov 27 2015, 00:32:21)
[GCC 4.8.4] on linux
Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.
&gt;&gt;&gt;
</code></pre>

<p>A noter que cette méthode fonctionne évidemment pour les autres versions de Python. Utile si vous devez tester votre code sur une version spécifique de Python non packagée dans votre système de paquet.</p>
                    </div>
                </section>
                
            </div>
            
<div class="pagination">
  <nav role="pagination" class="post-list-pagination">
      
    <span class="post-list-pagination-item post-list-pagination-item-current">Page 1 of 2</span>
    
      <a href="/posts/page/2/" class="post-list-pagination-item pure-button post-list-pagination-item-next">
        Older&nbsp;<i class="fa fa-angle-double-right"></i>
      </a>
    
  </nav>
</div>


            <div class="footer">
    <div class="pure-menu pure-menu-horizontal pure-menu-open">
        <ul>
            <li>Powered by <a class="hugo" href="https://gohugo.io/" target="_blank">hugo</a></li>
        </ul>
    </div>
</div>
<script src="https://ncrocfer.github.io/js/all.min.js"></script>

        </div>
    </div>
</div>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-65778407-1', 'auto');
ga('send', 'pageview');

</script>

</body>
</html>
